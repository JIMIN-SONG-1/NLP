{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsHTQVECT_5r"
      },
      "source": [
        "## [실습1] 시퀀스-투-시퀀스 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUSuJFgsh_mf"
      },
      "source": [
        "영어-스페인어 기계 번역과 영어-한국어 기계 번역을 작은 데이터셋을 이용하여 훈련시켜본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRw8L0jT_5r"
      },
      "source": [
        "### 영어-스페인어 기계 번역"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxuMxYWh_mj"
      },
      "source": [
        "**텍스트 데이터셋 다운로드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onFT02Hfh_mj"
      },
      "source": [
        "영어와 스페인어 텍스트가 담긴 압축 파일을 다운로드 한 후에 압축을 풀면\n",
        "\"spa.txt\" 파일이 생성된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4zqvFdKT_5r",
        "outputId": "1e97f4d7-d743-45d0-d29f-d94872e59484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-02 15:10:22--  https://www.manythings.org/anki/spa-eng.zip\n",
            "www.manythings.org (www.manythings.org) 해석 중... 173.254.30.110\n",
            "다음으로 연결 중: www.manythings.org (www.manythings.org)|173.254.30.110|:443... 연결했습니다.\n",
            "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
            "길이: 5420295 (5.2M) [application/zip]\n",
            "저장 위치: `spa-eng.zip'\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.17M  3.24MB/s    /  1.6s    \n",
            "\n",
            "2025-06-02 15:10:24 (3.24 MB/s) - `spa-eng.zip' 저장함 [5420295/5420295]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/manifests/1.25.0\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.8\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/blobs/sha256:d0d933dad3\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mwget\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/blobs/sha256:7fce09705a52a\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling wget dependency: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.8\u001b[0m\n",
            "Already downloaded: /Users/jamie/Library/Caches/Homebrew/downloads/f30f50fbde4bff9a71de54d684e482d7da3432656d680b97441163c6e5665468--libidn2-2.3.8.bottle_manifest.json\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPouring libidn2--2.3.8.arm64_ventura.bottle.tar.gz\u001b[0m\n",
            "🍺  /opt/homebrew/Cellar/libidn2/2.3.8: 80 files, 939.3KB\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mwget\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPouring wget--1.25.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
            "🍺  /opt/homebrew/Cellar/wget/1.25.0: 92 files, 4.5MB\n",
            "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup wget`...\u001b[0m\n",
            "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
            "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
          ]
        }
      ],
      "source": [
        "!brew install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpWMDbWh_mj"
      },
      "source": [
        "\"spa.txt\" 파일은 각각의 줄은 아래와 같이 영어 텍스트, 스페인어 텍스트, 기타 정보가 탭(tab) 키로 구분되어 있다.\n",
        "\n",
        "```\n",
        "Finally, it's Friday.\tAl fin es viernes.\tCC-BY 2.0 (France) Attribution: tatoeba.org #433868 (CK) & #1427385 (marcelostockle)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbW-WNK0h_mj"
      },
      "source": [
        "아래 코드는 \"spa.txt\"에 포함된 각 줄의 내용을 항목으로 갖는 리스트인 `text_pairs`를 생성한다.\n",
        "단 각 항목은 (영어 텍스트, 스페인어 텍스트)로 구성된 튜플이며, 각각의 줄에 포함된 기타 정보는 버린다.\n",
        "또한 스페인어 텍스트의 처음과 끝에 각각 `'[start] '` 와 `' [end]'`를 추가한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "adbnHDocT_5r"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish, _ = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJIGT5hh_mj"
      },
      "source": [
        "`text_pairs`에 포함된 임의의 항목을 확인하면 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_IurD9T_5s",
        "outputId": "9f1afe71-546f-4b11-ec09-f2eb089174ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('You should turn off your cell phone.', '[start] Debería apagar su teléfono móvil. [end]')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrVUHpZwh_mk"
      },
      "source": [
        "아래 코드는 텍스트를 무작위 섞은 다음\n",
        "70 대 15 대 15의 비율로 훈련 텍스트셋, 검증 텍스트셋, 테스트 텍스트셋으로 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kK2mwr96T_5s"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "# 검증셋 크기: 전체 데이터셋의 15%\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "# 훈련셋 크기: 전체 데이터셋의 70%\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "# 훈련 텍스트셋\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "# 검증 텍스트셋\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "# 테스트 텍스트셋\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuqUCPshT_5s"
      },
      "source": [
        "**영어/스페인어 텍스트 벡터화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvKYmmNZh_mk"
      },
      "source": [
        "자연어로 구성된 훈련 텍스트 데이터셋을 대상으로 어휘 인덱스를 생성한 후에 텍스트 벡터화를 진행한다.\n",
        "먼저 영어 어휘집을 생성한다.\n",
        "생성되는 어휘 벡터의 길이를 20으로 지정한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V8CWdFJIT_5t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:14:38.333133: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2025-06-02 15:14:38.333172: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2025-06-02 15:14:38.333178: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2025-06-02 15:14:38.333239: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-06-02 15:14:38.333284: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "2025-06-02 15:14:56.167356: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# 번역 대상 언어(예를 들어 영어) 텍스트 데이터셋 벡터화 층\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# 영어 텍스트만 추출\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "# 영어 어휘집 생성\n",
        "source_vectorization.adapt(train_english_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCqe3UYBh_mk"
      },
      "source": [
        "스페인어 텍스트 벡터화는 영어와는 다른 표준화 방식을 사용한다.\n",
        "\n",
        "- 영어에는 없는 `'¿'` 기호도 표준화 과정에서 삭제\n",
        "- 반면에 `'['`와 `']'`는 표준화 과정에서 제거되지 않도록 지정\n",
        "\n",
        "또한 생성되는 어휘 벡터의 길이를 21로 지정한다.\n",
        "그러면 0번 인덱스부터 19번 인덱스까지는 입력값으로,\n",
        "1번 인덱스부터 20번 인덱스까지는 타깃으로 지정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y_c1NpoVh_mk"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# 마침표 기호 목록에 \"¿\" 추가. 즉 표준화과정에서 삭제 대상으로 지정.\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "# 마침표 기호 목록으로부터 \"[\" 와 \"]\" 제거. 즉, 표준화 대상에서 삭제하지 않도록 함.\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# 새로운 표준화 함수 선언\n",
        "# 소문자로 변환한 후에 strip_chars 에 포함된 모든 기호 삭제\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# 번역 언어 (예를 들어 스페인어) 텍스트 데이터셋 벡터화 층\n",
        "# 벡터의 길이를 20이 아닌 21로 지정. 입력값과 타깃을 구분하기 위해 필요함.\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# 스페인어 텍스트만 추출\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# 스페인어 어휘집 생성\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7HeM8qPT_5t"
      },
      "source": [
        "훈련 텍스트셋, 검증 텍스트셋, 테스트 텍스트셋을 모두 어휘 인덱스를 이용하여 벡터화 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BKmph6Yh_ml"
      },
      "source": [
        "아래 코드는 생성된 영어와 스페인어 어휘 인덱스를 이용하여 각각의 텍스트 데이터셋을\n",
        "벡터화 한 다음에 아래 모양의 튜플로 구성된 훈련셋, 검증셋, 테스트셋을 생성한다.\n",
        "\n",
        "- 튜플의 첫째 항목: 영어 입력 배치와 스페인어 입력 배치로 구성된 사전. 모델의 입력값으로 사용.\n",
        "- 튜플의 둘째 항목: 타깃 배치. 모델 훈련의 타깃으로 사용.\n",
        "\n",
        "```\n",
        "({\"english\": 영어 입력 배치, \"spanish\": 스페인어 입력 배치}, 타깃 배치)\n",
        "```\n",
        "\n",
        "- `format_dataset()` 함수\n",
        "    - 인자: 영어 텍스트 배치와 스페인어 텍스트 배치\n",
        "    - 반환값: 앞서 언급한 모양의 사전\n",
        "    \n",
        "- `make_dataset()` 함수\n",
        "    - 인자: `(영어 텍스트, 스페인어 텍스트)` 모양의 튜플로 구성된 자연어 텍스트 데이터셋\\\n",
        "    - 반환값: 지정된 배치 크기로 묶은 배치들에 대해 `format_dataset()` 함수를 적용하여\n",
        "        생성된 `Dataset` 자료형의 데이터셋. 배치 단위로 묶여 있음.\n",
        "        - `dataset.shuffle(2048).prefetch(16).cache()`: 대용량 데이터셋을 배치 단위로\n",
        "            빠르게 불러오기 위해 사용함.        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XEgEfPpqT_5t"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\"english\": eng, \"spanish\": spa[:, :-1]}, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    dataset = dataset.map(format_dataset)\n",
        "\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "# 훈련셋\n",
        "train_ds = make_dataset(train_pairs)\n",
        "# 검증셋\n",
        "val_ds = make_dataset(val_pairs)\n",
        "# 테스트셋\n",
        "test_ds = make_dataset(test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh4ypb8Mh_ml"
      },
      "source": [
        "예를 들어 훈련셋의 첫째 배치의 모양을 확인하면 다음과 같다.\n",
        "\n",
        "- 영어 입력 배치: 길이가 20인 64개의 벡터로 구성. 즉, 20 개의 단어로 구성된 영어 텍스트 64개로 구성됨.\n",
        "- 스페인어 입력 배치: 길이가 20인 64개의 벡터로 구성. 즉, 20 개의 단어로 구성된 스페인어 텍스트 64개로 구성됨.\n",
        "- 타깃 배치: 길이가 20인 64개의 벡터로 구성. 즉, 20 개의 단어로 구성된 스페인어 텍스트 64개로 구성됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvL9A1MQT_5t",
        "outputId": "1db6f2f6-6ba9-406d-80f6-adc0ae681426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:16:50.909680: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zyx_f6mh_ml"
      },
      "source": [
        "아래 코드는 첫째 샘플의 영어 벡터, 스페인어 벡터, 타깃을 보여준다.\n",
        "스페인어 입력 벡터 샘플의 0번 인덱스에 위치한 정수 2가 `'[start]'`에 해당하는 값이다.\n",
        "스페인어 타깃 벡터 샘플은 그 값을 제외한 벡터로 시작함을 확인할 수 있다.\n",
        "또한 정수 3은 `'[end]'`에 해당하는 값이며, 문장의 끝을 가리키기에\n",
        "스페인어 타깃 벡터 샘플에 새로운 단어에 해당하는 인덱스를 추가하지 않고 대신 0 패딩이 하나 더 추가되었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F95a5ms4h_ml",
        "outputId": "1db6f2f6-6ba9-406d-80f6-adc0ae681426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 입력 벡터 샘플: [ 64  54   9  29 118   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "스페인어 입력 벡터 샘플: [   2  170 1199    3    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "스페인어 타깃 샘플: [ 170 1199    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:17:31.581907: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"영어 입력 벡터 샘플: {inputs['english'][0]}\")\n",
        "    print(f\"스페인어 입력 벡터 샘플: {inputs['spanish'][0]}\")\n",
        "    print(f\"스페인어 타깃 샘플: {targets[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JE4Oo1FT_5x"
      },
      "source": [
        "### 트랜스포머 디코더"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igiOWE0Rh_mn"
      },
      "source": [
        "트랜스포머 디코더를 하나의 층으로 구현하면 다음과 같다.\n",
        "생성자의 인자는 다음과 같다.\n",
        "\n",
        "- `embed_dim`: 예를 들어 `embed_dim=256`은 단어 임베딩 `(600, 256)` 모양의 샘플 생성\n",
        "- `dense_dim`: 밀집층에서 사용되는 유닛<font size='2'>unit</font> 개수\n",
        "- `num_heads`: 헤드<font size='2'>head</font> 개수\n",
        "\n",
        "`get_causal_attention_mask()` 메서드는 스페인어 입력 텍스트에 대한 마스크를 지정할 때 활용되지만\n",
        "여기서는 마스크를 사용하지 않는다.\n",
        "\n",
        "순전파를 담당하는 `call()` 메서드는 두 개의 어텐션 층을 사용한다.\n",
        "입력값으로는 스페인어 텍스트 배치 데이터셋과\n",
        "트랜스포머 디코더의 출력값으로 셀프 어텐션이 적용되어 변환된 영어 텍스트 배치 데이터셋이 사용된다.\n",
        "\n",
        "- `attention_1`: 스페인어 텍스트 입력값에 대해 셀프 어텐션 적용\n",
        "- `attention_2`: `attention_1` 의 출력값을 query로, 트랜스포머 인코더의 출력값을 key와 value로 사용해서 어텐션 적용.\n",
        "\n",
        "최종적으로 두 개의 밀집층을 통과시킨다.\n",
        "또한 하나의 블록을 통과시킬 때마다 잔차연결과 층정규화를 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.not_equal(inputs, 0)  # tf.not_equal 사용\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        \n",
        "        # 셀프 어텐션 적용\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        \n",
        "        # 피드포워드 네트워크 적용\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5I9kRgqdT_5x"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # 마스크 활용\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        # 셀프 어텐션 적용: 번역 언어(예를 들어 스페인어) 입력값 대상\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        # 셀프 어텐션이 적용된 (예를 들어 스페인어) 입력 텍스트를 query로\n",
        "        # 셀프 어텐션이 적용된 번역 대상 (예를 들어 영어) 입력 텍스트를 key와 value로\n",
        "        # 지정하여 어텐션 적용\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yw8lS_QT_5y"
      },
      "source": [
        "### 기계 번역 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Le1UYKh_mn"
      },
      "source": [
        "모델의 입력값은 앞서 설명한 대로 예를 들어 일정 길이로 단어 벡터화된 영어 텍스트 데이터셋과\n",
        "스페인어 텍스트 데이터셋의 튜플이다.\n",
        "스페인어 텍스트는 모두 `[start]` 로 시작하도록 전처리되어 있다.\n",
        "\n",
        "모델의 출력값은 예를 들어 출력 스페인어 텍스트로 지정될 단어들에 대한 위치별 확률값을 계산한다.\n",
        "아래 코드에서는 스페인어 텍스트에 포함될 20 개 단어들의 후보를 위치별로 확률값으로 계산한다.\n",
        "예를 들어 출력 텍스트의 i-번 인덱스에 위치할 단어의 확률값을 계산하기 위해\n",
        "어휘집에 포함된 15,000 개 단어를 대상으로 각각의 단어가 해당 위치에 자리할 확률을\n",
        "소프트맥스 함수를 이용하여 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W5B1RqwgT_5y"
      },
      "outputs": [],
      "source": [
        "sequence_length = 20 # 텍스트의 단어수\n",
        "vocab_size = 15000 # 어휘집 크기\n",
        "embed_dim = 256    # 단어 임베딩 크기\n",
        "dense_dim = 2048   # 밀집층 유닛수\n",
        "num_heads = 8      # 어텐션 헤드수\n",
        "\n",
        "# 트랜스포머 인코더 활용\n",
        "\n",
        "# 첫째 입력값: 예를 들어 영어 텍스트셋\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# 트랜스포머 디코더 활용\n",
        "\n",
        "# 둘째 입력값: 예를 들어 스페인어 텍스트셋\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53wCz6AT_5y"
      },
      "source": [
        "**모델 훈련과 활용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m97D2Hhhh_mn"
      },
      "source": [
        "모델의 최종 출력값이 소프트맥스를 사용하여\n",
        "`(20, 15000)` 모양을 갖는 반면에\n",
        "타깃셋은 20 개의 어휘 인덱스로 구성된 벡터로 구성되기에\n",
        "`categorical_crossentropy` 가 아닌 `sparse_categorical_crossentropy`를\n",
        "손실함수로 지정한다.\n",
        "그러면 20개 단어 각각에 대해 가장 높은 확률을 갖는 (어휘) 인덱스에 해당하는 단어가\n",
        "15,000 개 중에 선택되어 타깃 단어와 비교된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdbYo16QT_5y",
        "outputId": "494e1652-1a0f-440d-ec67-d77a613ed071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1549/1549 [==============================] - 1247s 802ms/step - loss: 4.0514 - accuracy: 0.4343 - val_loss: 3.2309 - val_accuracy: 0.5222\n",
            "Epoch 2/30\n",
            "1549/1549 [==============================] - 1184s 764ms/step - loss: 3.2023 - accuracy: 0.5484 - val_loss: 2.8252 - val_accuracy: 0.5797\n",
            "Epoch 3/30\n",
            "1549/1549 [==============================] - 1246s 804ms/step - loss: 2.8465 - accuracy: 0.5943 - val_loss: 2.6281 - val_accuracy: 0.6173\n",
            "Epoch 4/30\n",
            "1549/1549 [==============================] - 1170s 755ms/step - loss: 2.6730 - accuracy: 0.6233 - val_loss: 2.5349 - val_accuracy: 0.6341\n",
            "Epoch 5/30\n",
            "1549/1549 [==============================] - 1163s 751ms/step - loss: 2.5849 - accuracy: 0.6414 - val_loss: 2.5037 - val_accuracy: 0.6425\n",
            "Epoch 6/30\n",
            " 422/1549 [=======>......................] - ETA: 13:54 - loss: 2.5424 - accuracy: 0.6504"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_2veTqxT_5y"
      },
      "source": [
        "아래 `decode_sequence()`는 함수는 영어 텍스트가 하나 입력되면\n",
        "앞서 훈련된 트랜스포머 모델을 이용하여 지정된 길이인 20 개의 단어로\n",
        "구성된 스페인어 텍스트를 생성한다.\n",
        "\n",
        "함수 본문에 포함된 `for` 반복문은\n",
        "**트랜스포머 모델 활용** 부분에서 설명한 방식 그대로\n",
        "`[start]`로만 구성된 텍스트로 시작해서\n",
        "계속해서 텍스트에 추가할 단어를 하나씩 선택해서 이어가는 과정을\n",
        "`[end]` 키워드가 나올 때까지 반복한다.\n",
        "단, 반복횟수는 20으로 제한한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtl4ic-tT_5y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 어휘집 확인\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "# (단어 인덱스, 단어)로 구성된 사전 지정\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "# 텍스트에 포함되는 단어수\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    # 기계 번역 시작\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # 트랜스포머 모델 적용\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # i-번째 단어로 사용될 어휘 인덱스 확인\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # i-번째 단어 확인\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        # 스페인어 입력 텍스트에 i-번째 단어로 추가\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # 기계 번역 종료 조건 확인\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjro5CiWh_mo"
      },
      "source": [
        "아래 코드는 `decode_sequence()` 함수를 이용하여\n",
        "무작위로 5개의 영어 텍스트를 선택하여 기계 번역한 결과이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFJXavRueOZC",
        "outputId": "ca0b8631-40d7-452c-9893-f9689ffacb26",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
