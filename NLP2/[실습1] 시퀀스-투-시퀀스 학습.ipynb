{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsHTQVECT_5r"
      },
      "source": [
        "## [ì‹¤ìŠµ1] ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUSuJFgsh_mf"
      },
      "source": [
        "ì˜ì–´-ìŠ¤í˜ì¸ì–´ ê¸°ê³„ ë²ˆì—­ê³¼ ì˜ì–´-í•œêµ­ì–´ ê¸°ê³„ ë²ˆì—­ì„ ì‘ì€ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ í›ˆë ¨ì‹œì¼œë³¸ë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRw8L0jT_5r"
      },
      "source": [
        "### ì˜ì–´-ìŠ¤í˜ì¸ì–´ ê¸°ê³„ ë²ˆì—­"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxuMxYWh_mj"
      },
      "source": [
        "**í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onFT02Hfh_mj"
      },
      "source": [
        "ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ ì••ì¶• íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•œ í›„ì— ì••ì¶•ì„ í’€ë©´\n",
        "\"spa.txt\" íŒŒì¼ì´ ìƒì„±ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4zqvFdKT_5r",
        "outputId": "1e97f4d7-d743-45d0-d29f-d94872e59484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-02 15:10:22--  https://www.manythings.org/anki/spa-eng.zip\n",
            "www.manythings.org (www.manythings.org) í•´ì„ ì¤‘... 173.254.30.110\n",
            "ë‹¤ìŒìœ¼ë¡œ ì—°ê²° ì¤‘: www.manythings.org (www.manythings.org)|173.254.30.110|:443... ì—°ê²°í–ˆìŠµë‹ˆë‹¤.\n",
            "HTTP ìš”ì²­ì„ ë³´ëƒˆìŠµë‹ˆë‹¤. ì‘ë‹µ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘... 200 OK\n",
            "ê¸¸ì´: 5420295 (5.2M) [application/zip]\n",
            "ì €ì¥ ìœ„ì¹˜: `spa-eng.zip'\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.17M  3.24MB/s    /  1.6s    \n",
            "\n",
            "2025-06-02 15:10:24 (3.24 MB/s) - `spa-eng.zip' ì €ì¥í•¨ [5420295/5420295]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/manifests/1.25.0\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.8\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/blobs/sha256:d0d933dad3\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mwget\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/blobs/sha256:7fce09705a52a\u001b[0m\n",
            "######################################################################### 100.0%\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling dependencies for wget: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling wget dependency: \u001b[32mlibidn2\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/libidn2/manifests/2.3.8\u001b[0m\n",
            "Already downloaded: /Users/jamie/Library/Caches/Homebrew/downloads/f30f50fbde4bff9a71de54d684e482d7da3432656d680b97441163c6e5665468--libidn2-2.3.8.bottle_manifest.json\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPouring libidn2--2.3.8.arm64_ventura.bottle.tar.gz\u001b[0m\n",
            "ğŸº  /opt/homebrew/Cellar/libidn2/2.3.8: 80 files, 939.3KB\n",
            "\u001b[32m==>\u001b[0m \u001b[1mInstalling \u001b[32mwget\u001b[39m\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPouring wget--1.25.0.arm64_ventura.bottle.tar.gz\u001b[0m\n",
            "ğŸº  /opt/homebrew/Cellar/wget/1.25.0: 92 files, 4.5MB\n",
            "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup wget`...\u001b[0m\n",
            "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
            "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
          ]
        }
      ],
      "source": [
        "!brew install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpWMDbWh_mj"
      },
      "source": [
        "\"spa.txt\" íŒŒì¼ì€ ê°ê°ì˜ ì¤„ì€ ì•„ë˜ì™€ ê°™ì´ ì˜ì–´ í…ìŠ¤íŠ¸, ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸, ê¸°íƒ€ ì •ë³´ê°€ íƒ­(tab) í‚¤ë¡œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤.\n",
        "\n",
        "```\n",
        "Finally, it's Friday.\tAl fin es viernes.\tCC-BY 2.0 (France) Attribution: tatoeba.org #433868 (CK) & #1427385 (marcelostockle)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbW-WNK0h_mj"
      },
      "source": [
        "ì•„ë˜ ì½”ë“œëŠ” \"spa.txt\"ì— í¬í•¨ëœ ê° ì¤„ì˜ ë‚´ìš©ì„ í•­ëª©ìœ¼ë¡œ ê°–ëŠ” ë¦¬ìŠ¤íŠ¸ì¸ `text_pairs`ë¥¼ ìƒì„±í•œë‹¤.\n",
        "ë‹¨ ê° í•­ëª©ì€ (ì˜ì–´ í…ìŠ¤íŠ¸, ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸)ë¡œ êµ¬ì„±ëœ íŠœí”Œì´ë©°, ê°ê°ì˜ ì¤„ì— í¬í•¨ëœ ê¸°íƒ€ ì •ë³´ëŠ” ë²„ë¦°ë‹¤.\n",
        "ë˜í•œ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒê³¼ ëì— ê°ê° `'[start] '` ì™€ `' [end]'`ë¥¼ ì¶”ê°€í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "adbnHDocT_5r"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish, _ = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJIGT5hh_mj"
      },
      "source": [
        "`text_pairs`ì— í¬í•¨ëœ ì„ì˜ì˜ í•­ëª©ì„ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_IurD9T_5s",
        "outputId": "9f1afe71-546f-4b11-ec09-f2eb089174ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('You should turn off your cell phone.', '[start] DeberÃ­a apagar su telÃ©fono mÃ³vil. [end]')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrVUHpZwh_mk"
      },
      "source": [
        "ì•„ë˜ ì½”ë“œëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¬´ì‘ìœ„ ì„ì€ ë‹¤ìŒ\n",
        "70 ëŒ€ 15 ëŒ€ 15ì˜ ë¹„ìœ¨ë¡œ í›ˆë ¨ í…ìŠ¤íŠ¸ì…‹, ê²€ì¦ í…ìŠ¤íŠ¸ì…‹, í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kK2mwr96T_5s"
      },
      "outputs": [],
      "source": [
        "random.shuffle(text_pairs)\n",
        "\n",
        "# ê²€ì¦ì…‹ í¬ê¸°: ì „ì²´ ë°ì´í„°ì…‹ì˜ 15%\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "# í›ˆë ¨ì…‹ í¬ê¸°: ì „ì²´ ë°ì´í„°ì…‹ì˜ 70%\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "# í›ˆë ¨ í…ìŠ¤íŠ¸ì…‹\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "# ê²€ì¦ í…ìŠ¤íŠ¸ì…‹\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…‹\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuqUCPshT_5s"
      },
      "source": [
        "**ì˜ì–´/ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ë²¡í„°í™”**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvKYmmNZh_mk"
      },
      "source": [
        "ìì—°ì–´ë¡œ êµ¬ì„±ëœ í›ˆë ¨ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ ì–´íœ˜ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•œ í›„ì— í…ìŠ¤íŠ¸ ë²¡í„°í™”ë¥¼ ì§„í–‰í•œë‹¤.\n",
        "ë¨¼ì € ì˜ì–´ ì–´íœ˜ì§‘ì„ ìƒì„±í•œë‹¤.\n",
        "ìƒì„±ë˜ëŠ” ì–´íœ˜ ë²¡í„°ì˜ ê¸¸ì´ë¥¼ 20ìœ¼ë¡œ ì§€ì •í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V8CWdFJIT_5t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:14:38.333133: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2025-06-02 15:14:38.333172: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2025-06-02 15:14:38.333178: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2025-06-02 15:14:38.333239: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-06-02 15:14:38.333284: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "2025-06-02 15:14:56.167356: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# ë²ˆì—­ ëŒ€ìƒ ì–¸ì–´(ì˜ˆë¥¼ ë“¤ì–´ ì˜ì–´) í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë²¡í„°í™” ì¸µ\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "# ì˜ì–´ ì–´íœ˜ì§‘ ìƒì„±\n",
        "source_vectorization.adapt(train_english_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCqe3UYBh_mk"
      },
      "source": [
        "ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ë²¡í„°í™”ëŠ” ì˜ì–´ì™€ëŠ” ë‹¤ë¥¸ í‘œì¤€í™” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.\n",
        "\n",
        "- ì˜ì–´ì—ëŠ” ì—†ëŠ” `'Â¿'` ê¸°í˜¸ë„ í‘œì¤€í™” ê³¼ì •ì—ì„œ ì‚­ì œ\n",
        "- ë°˜ë©´ì— `'['`ì™€ `']'`ëŠ” í‘œì¤€í™” ê³¼ì •ì—ì„œ ì œê±°ë˜ì§€ ì•Šë„ë¡ ì§€ì •\n",
        "\n",
        "ë˜í•œ ìƒì„±ë˜ëŠ” ì–´íœ˜ ë²¡í„°ì˜ ê¸¸ì´ë¥¼ 21ë¡œ ì§€ì •í•œë‹¤.\n",
        "ê·¸ëŸ¬ë©´ 0ë²ˆ ì¸ë±ìŠ¤ë¶€í„° 19ë²ˆ ì¸ë±ìŠ¤ê¹Œì§€ëŠ” ì…ë ¥ê°’ìœ¼ë¡œ,\n",
        "1ë²ˆ ì¸ë±ìŠ¤ë¶€í„° 20ë²ˆ ì¸ë±ìŠ¤ê¹Œì§€ëŠ” íƒ€ê¹ƒìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y_c1NpoVh_mk"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# ë§ˆì¹¨í‘œ ê¸°í˜¸ ëª©ë¡ì— \"Â¿\" ì¶”ê°€. ì¦‰ í‘œì¤€í™”ê³¼ì •ì—ì„œ ì‚­ì œ ëŒ€ìƒìœ¼ë¡œ ì§€ì •.\n",
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "# ë§ˆì¹¨í‘œ ê¸°í˜¸ ëª©ë¡ìœ¼ë¡œë¶€í„° \"[\" ì™€ \"]\" ì œê±°. ì¦‰, í‘œì¤€í™” ëŒ€ìƒì—ì„œ ì‚­ì œí•˜ì§€ ì•Šë„ë¡ í•¨.\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# ìƒˆë¡œìš´ í‘œì¤€í™” í•¨ìˆ˜ ì„ ì–¸\n",
        "# ì†Œë¬¸ìë¡œ ë³€í™˜í•œ í›„ì— strip_chars ì— í¬í•¨ëœ ëª¨ë“  ê¸°í˜¸ ì‚­ì œ\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# ë²ˆì—­ ì–¸ì–´ (ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤í˜ì¸ì–´) í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë²¡í„°í™” ì¸µ\n",
        "# ë²¡í„°ì˜ ê¸¸ì´ë¥¼ 20ì´ ì•„ë‹Œ 21ë¡œ ì§€ì •. ì…ë ¥ê°’ê³¼ íƒ€ê¹ƒì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ í•„ìš”í•¨.\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# ìŠ¤í˜ì¸ì–´ ì–´íœ˜ì§‘ ìƒì„±\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7HeM8qPT_5t"
      },
      "source": [
        "í›ˆë ¨ í…ìŠ¤íŠ¸ì…‹, ê²€ì¦ í…ìŠ¤íŠ¸ì…‹, í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…‹ì„ ëª¨ë‘ ì–´íœ˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ë²¡í„°í™” í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BKmph6Yh_ml"
      },
      "source": [
        "ì•„ë˜ ì½”ë“œëŠ” ìƒì„±ëœ ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ ì–´íœ˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ê°ê°ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„\n",
        "ë²¡í„°í™” í•œ ë‹¤ìŒì— ì•„ë˜ ëª¨ì–‘ì˜ íŠœí”Œë¡œ êµ¬ì„±ëœ í›ˆë ¨ì…‹, ê²€ì¦ì…‹, í…ŒìŠ¤íŠ¸ì…‹ì„ ìƒì„±í•œë‹¤.\n",
        "\n",
        "- íŠœí”Œì˜ ì²«ì§¸ í•­ëª©: ì˜ì–´ ì…ë ¥ ë°°ì¹˜ì™€ ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë°°ì¹˜ë¡œ êµ¬ì„±ëœ ì‚¬ì „. ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©.\n",
        "- íŠœí”Œì˜ ë‘˜ì§¸ í•­ëª©: íƒ€ê¹ƒ ë°°ì¹˜. ëª¨ë¸ í›ˆë ¨ì˜ íƒ€ê¹ƒìœ¼ë¡œ ì‚¬ìš©.\n",
        "\n",
        "```\n",
        "({\"english\": ì˜ì–´ ì…ë ¥ ë°°ì¹˜, \"spanish\": ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë°°ì¹˜}, íƒ€ê¹ƒ ë°°ì¹˜)\n",
        "```\n",
        "\n",
        "- `format_dataset()` í•¨ìˆ˜\n",
        "    - ì¸ì: ì˜ì–´ í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ë°°ì¹˜\n",
        "    - ë°˜í™˜ê°’: ì•ì„œ ì–¸ê¸‰í•œ ëª¨ì–‘ì˜ ì‚¬ì „\n",
        "    \n",
        "- `make_dataset()` í•¨ìˆ˜\n",
        "    - ì¸ì: `(ì˜ì–´ í…ìŠ¤íŠ¸, ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸)` ëª¨ì–‘ì˜ íŠœí”Œë¡œ êµ¬ì„±ëœ ìì—°ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹\\\n",
        "    - ë°˜í™˜ê°’: ì§€ì •ëœ ë°°ì¹˜ í¬ê¸°ë¡œ ë¬¶ì€ ë°°ì¹˜ë“¤ì— ëŒ€í•´ `format_dataset()` í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬\n",
        "        ìƒì„±ëœ `Dataset` ìë£Œí˜•ì˜ ë°ì´í„°ì…‹. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì—¬ ìˆìŒ.\n",
        "        - `dataset.shuffle(2048).prefetch(16).cache()`: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ\n",
        "            ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©í•¨.        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XEgEfPpqT_5t"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\"english\": eng, \"spanish\": spa[:, :-1]}, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    dataset = dataset.map(format_dataset)\n",
        "\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "# í›ˆë ¨ì…‹\n",
        "train_ds = make_dataset(train_pairs)\n",
        "# ê²€ì¦ì…‹\n",
        "val_ds = make_dataset(val_pairs)\n",
        "# í…ŒìŠ¤íŠ¸ì…‹\n",
        "test_ds = make_dataset(test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh4ypb8Mh_ml"
      },
      "source": [
        "ì˜ˆë¥¼ ë“¤ì–´ í›ˆë ¨ì…‹ì˜ ì²«ì§¸ ë°°ì¹˜ì˜ ëª¨ì–‘ì„ í™•ì¸í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
        "\n",
        "- ì˜ì–´ ì…ë ¥ ë°°ì¹˜: ê¸¸ì´ê°€ 20ì¸ 64ê°œì˜ ë²¡í„°ë¡œ êµ¬ì„±. ì¦‰, 20 ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ì˜ì–´ í…ìŠ¤íŠ¸ 64ê°œë¡œ êµ¬ì„±ë¨.\n",
        "- ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë°°ì¹˜: ê¸¸ì´ê°€ 20ì¸ 64ê°œì˜ ë²¡í„°ë¡œ êµ¬ì„±. ì¦‰, 20 ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ 64ê°œë¡œ êµ¬ì„±ë¨.\n",
        "- íƒ€ê¹ƒ ë°°ì¹˜: ê¸¸ì´ê°€ 20ì¸ 64ê°œì˜ ë²¡í„°ë¡œ êµ¬ì„±. ì¦‰, 20 ê°œì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ 64ê°œë¡œ êµ¬ì„±ë¨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvL9A1MQT_5t",
        "outputId": "1db6f2f6-6ba9-406d-80f6-adc0ae681426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:16:50.909680: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zyx_f6mh_ml"
      },
      "source": [
        "ì•„ë˜ ì½”ë“œëŠ” ì²«ì§¸ ìƒ˜í”Œì˜ ì˜ì–´ ë²¡í„°, ìŠ¤í˜ì¸ì–´ ë²¡í„°, íƒ€ê¹ƒì„ ë³´ì—¬ì¤€ë‹¤.\n",
        "ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë²¡í„° ìƒ˜í”Œì˜ 0ë²ˆ ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ì •ìˆ˜ 2ê°€ `'[start]'`ì— í•´ë‹¹í•˜ëŠ” ê°’ì´ë‹¤.\n",
        "ìŠ¤í˜ì¸ì–´ íƒ€ê¹ƒ ë²¡í„° ìƒ˜í”Œì€ ê·¸ ê°’ì„ ì œì™¸í•œ ë²¡í„°ë¡œ ì‹œì‘í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "ë˜í•œ ì •ìˆ˜ 3ì€ `'[end]'`ì— í•´ë‹¹í•˜ëŠ” ê°’ì´ë©°, ë¬¸ì¥ì˜ ëì„ ê°€ë¦¬í‚¤ê¸°ì—\n",
        "ìŠ¤í˜ì¸ì–´ íƒ€ê¹ƒ ë²¡í„° ìƒ˜í”Œì— ìƒˆë¡œìš´ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šê³  ëŒ€ì‹  0 íŒ¨ë”©ì´ í•˜ë‚˜ ë” ì¶”ê°€ë˜ì—ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F95a5ms4h_ml",
        "outputId": "1db6f2f6-6ba9-406d-80f6-adc0ae681426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì˜ì–´ ì…ë ¥ ë²¡í„° ìƒ˜í”Œ: [ 64  54   9  29 118   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë²¡í„° ìƒ˜í”Œ: [   2  170 1199    3    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "ìŠ¤í˜ì¸ì–´ íƒ€ê¹ƒ ìƒ˜í”Œ: [ 170 1199    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-02 15:17:31.581907: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"ì˜ì–´ ì…ë ¥ ë²¡í„° ìƒ˜í”Œ: {inputs['english'][0]}\")\n",
        "    print(f\"ìŠ¤í˜ì¸ì–´ ì…ë ¥ ë²¡í„° ìƒ˜í”Œ: {inputs['spanish'][0]}\")\n",
        "    print(f\"ìŠ¤í˜ì¸ì–´ íƒ€ê¹ƒ ìƒ˜í”Œ: {targets[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JE4Oo1FT_5x"
      },
      "source": [
        "### íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igiOWE0Rh_mn"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ë¥¼ í•˜ë‚˜ì˜ ì¸µìœ¼ë¡œ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
        "ìƒì„±ìì˜ ì¸ìëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
        "\n",
        "- `embed_dim`: ì˜ˆë¥¼ ë“¤ì–´ `embed_dim=256`ì€ ë‹¨ì–´ ì„ë² ë”© `(600, 256)` ëª¨ì–‘ì˜ ìƒ˜í”Œ ìƒì„±\n",
        "- `dense_dim`: ë°€ì§‘ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” ìœ ë‹›<font size='2'>unit</font> ê°œìˆ˜\n",
        "- `num_heads`: í—¤ë“œ<font size='2'>head</font> ê°œìˆ˜\n",
        "\n",
        "`get_causal_attention_mask()` ë©”ì„œë“œëŠ” ìŠ¤í˜ì¸ì–´ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë§ˆìŠ¤í¬ë¥¼ ì§€ì •í•  ë•Œ í™œìš©ë˜ì§€ë§Œ\n",
        "ì—¬ê¸°ì„œëŠ” ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "\n",
        "ìˆœì „íŒŒë¥¼ ë‹´ë‹¹í•˜ëŠ” `call()` ë©”ì„œë“œëŠ” ë‘ ê°œì˜ ì–´í…ì…˜ ì¸µì„ ì‚¬ìš©í•œë‹¤.\n",
        "ì…ë ¥ê°’ìœ¼ë¡œëŠ” ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ë°°ì¹˜ ë°ì´í„°ì…‹ê³¼\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ì˜ ì¶œë ¥ê°’ìœ¼ë¡œ ì…€í”„ ì–´í…ì…˜ì´ ì ìš©ë˜ì–´ ë³€í™˜ëœ ì˜ì–´ í…ìŠ¤íŠ¸ ë°°ì¹˜ ë°ì´í„°ì…‹ì´ ì‚¬ìš©ëœë‹¤.\n",
        "\n",
        "- `attention_1`: ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ì…ë ¥ê°’ì— ëŒ€í•´ ì…€í”„ ì–´í…ì…˜ ì ìš©\n",
        "- `attention_2`: `attention_1` ì˜ ì¶œë ¥ê°’ì„ queryë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ì¶œë ¥ê°’ì„ keyì™€ valueë¡œ ì‚¬ìš©í•´ì„œ ì–´í…ì…˜ ì ìš©.\n",
        "\n",
        "ìµœì¢…ì ìœ¼ë¡œ ë‘ ê°œì˜ ë°€ì§‘ì¸µì„ í†µê³¼ì‹œí‚¨ë‹¤.\n",
        "ë˜í•œ í•˜ë‚˜ì˜ ë¸”ë¡ì„ í†µê³¼ì‹œí‚¬ ë•Œë§ˆë‹¤ ì”ì°¨ì—°ê²°ê³¼ ì¸µì •ê·œí™”ë¥¼ ì§„í–‰í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.not_equal(inputs, 0)  # tf.not_equal ì‚¬ìš©\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        \n",
        "        # ì…€í”„ ì–´í…ì…˜ ì ìš©\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        \n",
        "        # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì ìš©\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5I9kRgqdT_5x"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # ë§ˆìŠ¤í¬ í™œìš©\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        # ì…€í”„ ì–´í…ì…˜ ì ìš©: ë²ˆì—­ ì–¸ì–´(ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤í˜ì¸ì–´) ì…ë ¥ê°’ ëŒ€ìƒ\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        # ì…€í”„ ì–´í…ì…˜ì´ ì ìš©ëœ (ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤í˜ì¸ì–´) ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ queryë¡œ\n",
        "        # ì…€í”„ ì–´í…ì…˜ì´ ì ìš©ëœ ë²ˆì—­ ëŒ€ìƒ (ì˜ˆë¥¼ ë“¤ì–´ ì˜ì–´) ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ keyì™€ valueë¡œ\n",
        "        # ì§€ì •í•˜ì—¬ ì–´í…ì…˜ ì ìš©\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yw8lS_QT_5y"
      },
      "source": [
        "### ê¸°ê³„ ë²ˆì—­ ëª¨ë¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Le1UYKh_mn"
      },
      "source": [
        "ëª¨ë¸ì˜ ì…ë ¥ê°’ì€ ì•ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ì˜ˆë¥¼ ë“¤ì–´ ì¼ì • ê¸¸ì´ë¡œ ë‹¨ì–´ ë²¡í„°í™”ëœ ì˜ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼\n",
        "ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ íŠœí”Œì´ë‹¤.\n",
        "ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ëŠ” ëª¨ë‘ `[start]` ë¡œ ì‹œì‘í•˜ë„ë¡ ì „ì²˜ë¦¬ë˜ì–´ ìˆë‹¤.\n",
        "\n",
        "ëª¨ë¸ì˜ ì¶œë ¥ê°’ì€ ì˜ˆë¥¼ ë“¤ì–´ ì¶œë ¥ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ë¡œ ì§€ì •ë  ë‹¨ì–´ë“¤ì— ëŒ€í•œ ìœ„ì¹˜ë³„ í™•ë¥ ê°’ì„ ê³„ì‚°í•œë‹¤.\n",
        "ì•„ë˜ ì½”ë“œì—ì„œëŠ” ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ì— í¬í•¨ë  20 ê°œ ë‹¨ì–´ë“¤ì˜ í›„ë³´ë¥¼ ìœ„ì¹˜ë³„ë¡œ í™•ë¥ ê°’ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.\n",
        "ì˜ˆë¥¼ ë“¤ì–´ ì¶œë ¥ í…ìŠ¤íŠ¸ì˜ i-ë²ˆ ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•  ë‹¨ì–´ì˜ í™•ë¥ ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´\n",
        "ì–´íœ˜ì§‘ì— í¬í•¨ëœ 15,000 ê°œ ë‹¨ì–´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê°ê°ì˜ ë‹¨ì–´ê°€ í•´ë‹¹ ìœ„ì¹˜ì— ìë¦¬í•  í™•ë¥ ì„\n",
        "ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê³„ì‚°í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W5B1RqwgT_5y"
      },
      "outputs": [],
      "source": [
        "sequence_length = 20 # í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ìˆ˜\n",
        "vocab_size = 15000 # ì–´íœ˜ì§‘ í¬ê¸°\n",
        "embed_dim = 256    # ë‹¨ì–´ ì„ë² ë”© í¬ê¸°\n",
        "dense_dim = 2048   # ë°€ì§‘ì¸µ ìœ ë‹›ìˆ˜\n",
        "num_heads = 8      # ì–´í…ì…˜ í—¤ë“œìˆ˜\n",
        "\n",
        "# íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í™œìš©\n",
        "\n",
        "# ì²«ì§¸ ì…ë ¥ê°’: ì˜ˆë¥¼ ë“¤ì–´ ì˜ì–´ í…ìŠ¤íŠ¸ì…‹\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” í™œìš©\n",
        "\n",
        "# ë‘˜ì§¸ ì…ë ¥ê°’: ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ì…‹\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53wCz6AT_5y"
      },
      "source": [
        "**ëª¨ë¸ í›ˆë ¨ê³¼ í™œìš©**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m97D2Hhhh_mn"
      },
      "source": [
        "ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ê°’ì´ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
        "`(20, 15000)` ëª¨ì–‘ì„ ê°–ëŠ” ë°˜ë©´ì—\n",
        "íƒ€ê¹ƒì…‹ì€ 20 ê°œì˜ ì–´íœ˜ ì¸ë±ìŠ¤ë¡œ êµ¬ì„±ëœ ë²¡í„°ë¡œ êµ¬ì„±ë˜ê¸°ì—\n",
        "`categorical_crossentropy` ê°€ ì•„ë‹Œ `sparse_categorical_crossentropy`ë¥¼\n",
        "ì†ì‹¤í•¨ìˆ˜ë¡œ ì§€ì •í•œë‹¤.\n",
        "ê·¸ëŸ¬ë©´ 20ê°œ ë‹¨ì–´ ê°ê°ì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°–ëŠ” (ì–´íœ˜) ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€\n",
        "15,000 ê°œ ì¤‘ì— ì„ íƒë˜ì–´ íƒ€ê¹ƒ ë‹¨ì–´ì™€ ë¹„êµëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdbYo16QT_5y",
        "outputId": "494e1652-1a0f-440d-ec67-d77a613ed071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1549/1549 [==============================] - 1247s 802ms/step - loss: 4.0514 - accuracy: 0.4343 - val_loss: 3.2309 - val_accuracy: 0.5222\n",
            "Epoch 2/30\n",
            "1549/1549 [==============================] - 1184s 764ms/step - loss: 3.2023 - accuracy: 0.5484 - val_loss: 2.8252 - val_accuracy: 0.5797\n",
            "Epoch 3/30\n",
            "1549/1549 [==============================] - 1246s 804ms/step - loss: 2.8465 - accuracy: 0.5943 - val_loss: 2.6281 - val_accuracy: 0.6173\n",
            "Epoch 4/30\n",
            "1549/1549 [==============================] - 1170s 755ms/step - loss: 2.6730 - accuracy: 0.6233 - val_loss: 2.5349 - val_accuracy: 0.6341\n",
            "Epoch 5/30\n",
            "1549/1549 [==============================] - 1163s 751ms/step - loss: 2.5849 - accuracy: 0.6414 - val_loss: 2.5037 - val_accuracy: 0.6425\n",
            "Epoch 6/30\n",
            " 422/1549 [=======>......................] - ETA: 13:54 - loss: 2.5424 - accuracy: 0.6504"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
            "File \u001b[0;32m~/Desktop/NLP-practice2-main 2/NLP2/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_2veTqxT_5y"
      },
      "source": [
        "ì•„ë˜ `decode_sequence()`ëŠ” í•¨ìˆ˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸ê°€ í•˜ë‚˜ ì…ë ¥ë˜ë©´\n",
        "ì•ì„œ í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì§€ì •ëœ ê¸¸ì´ì¸ 20 ê°œì˜ ë‹¨ì–´ë¡œ\n",
        "êµ¬ì„±ëœ ìŠ¤í˜ì¸ì–´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.\n",
        "\n",
        "í•¨ìˆ˜ ë³¸ë¬¸ì— í¬í•¨ëœ `for` ë°˜ë³µë¬¸ì€\n",
        "**íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í™œìš©** ë¶€ë¶„ì—ì„œ ì„¤ëª…í•œ ë°©ì‹ ê·¸ëŒ€ë¡œ\n",
        "`[start]`ë¡œë§Œ êµ¬ì„±ëœ í…ìŠ¤íŠ¸ë¡œ ì‹œì‘í•´ì„œ\n",
        "ê³„ì†í•´ì„œ í…ìŠ¤íŠ¸ì— ì¶”ê°€í•  ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ì„ íƒí•´ì„œ ì´ì–´ê°€ëŠ” ê³¼ì •ì„\n",
        "`[end]` í‚¤ì›Œë“œê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.\n",
        "ë‹¨, ë°˜ë³µíšŸìˆ˜ëŠ” 20ìœ¼ë¡œ ì œí•œí•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtl4ic-tT_5y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ì–´íœ˜ì§‘ í™•ì¸\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "# (ë‹¨ì–´ ì¸ë±ìŠ¤, ë‹¨ì–´)ë¡œ êµ¬ì„±ëœ ì‚¬ì „ ì§€ì •\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "# í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ” ë‹¨ì–´ìˆ˜\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    # ê¸°ê³„ ë²ˆì—­ ì‹œì‘\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ì ìš©\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # i-ë²ˆì§¸ ë‹¨ì–´ë¡œ ì‚¬ìš©ë  ì–´íœ˜ ì¸ë±ìŠ¤ í™•ì¸\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # i-ë²ˆì§¸ ë‹¨ì–´ í™•ì¸\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        # ìŠ¤í˜ì¸ì–´ ì…ë ¥ í…ìŠ¤íŠ¸ì— i-ë²ˆì§¸ ë‹¨ì–´ë¡œ ì¶”ê°€\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # ê¸°ê³„ ë²ˆì—­ ì¢…ë£Œ ì¡°ê±´ í™•ì¸\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjro5CiWh_mo"
      },
      "source": [
        "ì•„ë˜ ì½”ë“œëŠ” `decode_sequence()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬\n",
        "ë¬´ì‘ìœ„ë¡œ 5ê°œì˜ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ê¸°ê³„ ë²ˆì—­í•œ ê²°ê³¼ì´ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFJXavRueOZC",
        "outputId": "ca0b8631-40d7-452c-9893-f9689ffacb26",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
